{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db2abde-1118-42e8-a624-d42bb5014ebf",
   "metadata": {},
   "source": [
    "# Generate Bat Outcome\n",
    " - Given the batter has made contact on the given pitch characteristics, what is the batting outcome?\n",
    " - I.e., what is the launch_speed_angle, and what is the hit_location\n",
    "\n",
    "## Potential Difficulties:\n",
    " - How to factor in bunting?\n",
    "\n",
    "$$\\alpha$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc14a55d-d41b-49b6-8d57-4a6ed8ecd1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.features import build_features as f\n",
    "from src.data import data_utils as du\n",
    "from importlib import reload\n",
    "reload(f)\n",
    "\n",
    "vladdy = 665489\n",
    "soto = 665742\n",
    "schneider = 676914\n",
    "biggio = 624415\n",
    "showtime = 660271\n",
    "batter = vladdy\n",
    "X_train, y_train, X_test, y_test, encoders = f.get_pitch_outcome_dataset_xgb(batter, split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "883d3156-b35f-4c3d-8190-b12272df3124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            \n",
       "ball             3956\n",
       "strike           2709\n",
       "hit_into_play    2117\n",
       "foul             1760\n",
       "hit_by_pitch       26\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(encoders['pitch_outcome'].inverse_transform(y_train['pitch_outcome'])).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88365f8e-b9a9-436d-9c10-9adb8daf54ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchbnn as bnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4cc5f74-cf19-4810-a14b-2b8989d3b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandwidth: scott, Average Log Likelihood: -346.7922063927979\n",
      "Bandwidth: silverman, Average Log Likelihood: -347.29842144009\n",
      "Bandwidth: 0.1, Average Log Likelihood: -347.3189852133204\n",
      "Bandwidth: 0.2, Average Log Likelihood: -347.17586592870134\n",
      "Bandwidth: 0.5, Average Log Likelihood: -348.48462433847527\n",
      "Bandwidth: 1.0, Average Log Likelihood: -354.17598208015966\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def evaluate_kde_bandwidth(dataframe, bandwidth):\n",
    "    kde_estimators = []\n",
    "    inverse_cdfs = []\n",
    "    for column in dataframe.columns:\n",
    "        kde = gaussian_kde(dataframe[column], bw_method=bandwidth)\n",
    "        kde_estimators.append(kde)\n",
    "        \n",
    "        # Compute the inverse CDF using interpolation\n",
    "        x_vals = np.linspace(dataframe[column].min(), dataframe[column].max(), 1000)\n",
    "        cdf_vals = np.array([kde.integrate_box_1d(-np.inf, x) for x in x_vals])\n",
    "        inverse_cdf = interp1d(cdf_vals, x_vals, bounds_error=False, fill_value=(x_vals[0], x_vals[-1]))\n",
    "        inverse_cdfs.append(inverse_cdf)\n",
    "    \n",
    "    return kde_estimators, inverse_cdfs\n",
    "\n",
    "def generate_gaussian_copula_kde(dataframe, kde_estimators, inverse_cdfs, n_samples=None):\n",
    "    if n_samples is None:\n",
    "        n_samples = len(dataframe)\n",
    "    \n",
    "    # Step 2: Transform data to uniform marginals using the CDF of KDEs\n",
    "    uniform_marginals = np.zeros_like(dataframe.values)\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        cdf_values = np.array([kde_estimators[i].integrate_box_1d(-np.inf, x) for x in dataframe[column]])\n",
    "        uniform_marginals[:, i] = cdf_values\n",
    "    \n",
    "    # Step 3: Transform uniform marginals to standard normal marginals\n",
    "    normal_marginals = norm.ppf(uniform_marginals)\n",
    "    \n",
    "    # Ensure no infinite values (can occur if uniform marginals are exactly 0 or 1)\n",
    "    normal_marginals[np.isinf(normal_marginals)] = np.nan\n",
    "    normal_marginals = np.nan_to_num(normal_marginals)\n",
    "    \n",
    "    # Step 4: Generate new samples from a multivariate normal distribution\n",
    "    mean = np.zeros(dataframe.shape[1])\n",
    "    cov = np.corrcoef(normal_marginals, rowvar=False)\n",
    "    new_samples_normal = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "    \n",
    "    # Step 5: Transform new samples from standard normal to uniform marginals\n",
    "    new_samples_uniform = norm.cdf(new_samples_normal)\n",
    "    \n",
    "    # Step 6: Transform uniform marginals back to original KDE marginals\n",
    "    new_samples = np.zeros_like(new_samples_uniform)\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        new_samples[:, i] = inverse_cdfs[i](new_samples_uniform[:, i])\n",
    "    \n",
    "    # Create a DataFrame with the new samples\n",
    "    generated_dataframe = pd.DataFrame(new_samples, columns=dataframe.columns)\n",
    "    \n",
    "    return generated_dataframe\n",
    "\n",
    "def cross_validate_kde(data, bandwidths, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    results = []\n",
    "    \n",
    "    for bandwidth in bandwidths:\n",
    "        log_likelihoods = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(data):\n",
    "            train_data, test_data = data.iloc[train_index], data.iloc[test_index]\n",
    "            kde_estimators, inverse_cdfs = evaluate_kde_bandwidth(train_data, bandwidth)\n",
    "            generated_data = generate_gaussian_copula_kde(train_data, kde_estimators, inverse_cdfs, n_samples=len(test_data))\n",
    "            \n",
    "            # Evaluate the generated data\n",
    "            for column in data.columns:\n",
    "                kde = gaussian_kde(generated_data[column])\n",
    "                log_likelihood = kde.logpdf(test_data[column]).sum()\n",
    "                log_likelihoods.append(log_likelihood)\n",
    "        \n",
    "        avg_log_likelihood = np.mean(log_likelihoods)\n",
    "        results.append((bandwidth, avg_log_likelihood))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "bandwidths = ['scott', 'silverman', 0.1, 0.2, 0.5, 1.0]\n",
    "results = cross_validate_kde(y_train, bandwidths)\n",
    "\n",
    "for bandwidth, avg_log_likelihood in results:\n",
    "    print(f'Bandwidth: {bandwidth}, Average Log Likelihood: {avg_log_likelihood}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64097c09-7fd9-4ed6-a309-4c5f296602d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bandwidth: 0.1\n",
      "Test Set Log Likelihood: -198.44203485972176\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def evaluate_kde_bandwidth(dataframe, bandwidth):\n",
    "    kde_estimators = []\n",
    "    inverse_cdfs = []\n",
    "    for column in dataframe.columns:\n",
    "        kde = gaussian_kde(dataframe[column], bw_method=bandwidth)\n",
    "        kde_estimators.append(kde)\n",
    "        \n",
    "        # Compute the inverse CDF using interpolation\n",
    "        x_vals = np.linspace(dataframe[column].min(), dataframe[column].max(), 1000)\n",
    "        cdf_vals = np.array([kde.integrate_box_1d(-np.inf, x) for x in x_vals])\n",
    "        inverse_cdf = interp1d(cdf_vals, x_vals, bounds_error=False, fill_value=(x_vals[0], x_vals[-1]))\n",
    "        inverse_cdfs.append(inverse_cdf)\n",
    "    \n",
    "    return kde_estimators, inverse_cdfs\n",
    "\n",
    "def generate_gaussian_copula_kde(dataframe, kde_estimators, inverse_cdfs, n_samples=None):\n",
    "    if n_samples is None:\n",
    "        n_samples = len(dataframe)\n",
    "    \n",
    "    # Step 2: Transform data to uniform marginals using the CDF of KDEs\n",
    "    uniform_marginals = np.zeros_like(dataframe.values)\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        cdf_values = np.array([kde_estimators[i].integrate_box_1d(-np.inf, x) for x in dataframe[column]])\n",
    "        uniform_marginals[:, i] = cdf_values\n",
    "    \n",
    "    # Step 3: Transform uniform marginals to standard normal marginals\n",
    "    normal_marginals = norm.ppf(uniform_marginals)\n",
    "    \n",
    "    # Ensure no infinite values (can occur if uniform marginals are exactly 0 or 1)\n",
    "    normal_marginals[np.isinf(normal_marginals)] = np.nan\n",
    "    normal_marginals = np.nan_to_num(normal_marginals)\n",
    "    \n",
    "    # Step 4: Generate new samples from a multivariate normal distribution\n",
    "    mean = np.zeros(dataframe.shape[1])\n",
    "    cov = np.corrcoef(normal_marginals, rowvar=False)\n",
    "    new_samples_normal = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "    \n",
    "    # Step 5: Transform new samples from standard normal to uniform marginals\n",
    "    new_samples_uniform = norm.cdf(new_samples_normal)\n",
    "    \n",
    "    # Step 6: Transform uniform marginals back to original KDE marginals\n",
    "    new_samples = np.zeros_like(new_samples_uniform)\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        new_samples[:, i] = inverse_cdfs[i](new_samples_uniform[:, i])\n",
    "    \n",
    "    # Create a DataFrame with the new samples\n",
    "    generated_dataframe = pd.DataFrame(new_samples, columns=dataframe.columns)\n",
    "    \n",
    "    return generated_dataframe\n",
    "\n",
    "def cross_validate_kde(data, bandwidths, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    results = []\n",
    "    \n",
    "    for bandwidth in bandwidths:\n",
    "        train_log_likelihoods = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(data):\n",
    "            train_data = data.iloc[train_index]\n",
    "            kde_estimators, inverse_cdfs = evaluate_kde_bandwidth(train_data, bandwidth)\n",
    "            generated_data = generate_gaussian_copula_kde(train_data, kde_estimators, inverse_cdfs, n_samples=len(train_data))\n",
    "            \n",
    "            # Evaluate the generated data\n",
    "            log_likelihoods = []\n",
    "            for column in data.columns:\n",
    "                kde = gaussian_kde(generated_data[column])\n",
    "                log_likelihood = kde.logpdf(train_data[column]).sum()\n",
    "                log_likelihoods.append(log_likelihood)\n",
    "            train_log_likelihoods.append(np.mean(log_likelihoods))\n",
    "        \n",
    "        avg_train_log_likelihood = np.mean(train_log_likelihoods)\n",
    "        results.append((bandwidth, avg_train_log_likelihood))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_on_test_set(data, test_data, best_bandwidth):\n",
    "    kde_estimators, inverse_cdfs = evaluate_kde_bandwidth(data, best_bandwidth)\n",
    "    generated_data = generate_gaussian_copula_kde(data, kde_estimators, inverse_cdfs, n_samples=len(test_data))\n",
    "    \n",
    "    log_likelihoods = []\n",
    "    for column in test_data.columns:\n",
    "        kde = gaussian_kde(generated_data[column])\n",
    "        log_likelihood = kde.logpdf(test_data[column]).sum()\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "    \n",
    "    avg_test_log_likelihood = np.mean(log_likelihoods)\n",
    "    return avg_test_log_likelihood\n",
    "\n",
    "\n",
    "bandwidths = ['scott', 'silverman', 0.1, 0.2, 0.5, 1.0]\n",
    "results = cross_validate_kde(y_train, bandwidths)\n",
    "\n",
    "# Find the best bandwidth based on the highest average log likelihood\n",
    "best_bandwidth = max(results, key=lambda x: x[1])[0]\n",
    "print(f'Best Bandwidth: {best_bandwidth}')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_log_likelihood = evaluate_on_test_set(y_train, y_test, best_bandwidth)\n",
    "print(f'Test Set Log Likelihood: {test_log_likelihood}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5df4d6b-dc1a-4e0b-bd11-372e433ca809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       launch_speed  launch_angle  spray_angle\n",
      "count    391.000000    391.000000   391.000000\n",
      "mean      94.802302     13.726343    87.273657\n",
      "std       14.500436     25.429325    23.123177\n",
      "min       48.000000    -73.000000    49.000000\n",
      "25%       84.100000     -2.000000    69.000000\n",
      "50%       97.400000     15.000000    82.000000\n",
      "75%      106.300000     29.000000   105.000000\n",
      "max      119.200000     82.000000   162.000000\n",
      "       launch_speed  launch_angle  spray_angle\n",
      "count   1000.000000   1000.000000  1000.000000\n",
      "mean      94.114076     13.840558    87.269046\n",
      "std       15.608933     26.318241    23.987326\n",
      "min       48.000000    -73.000000    49.000000\n",
      "25%       83.058173     -0.889500    68.263511\n",
      "50%       95.969965     14.273814    82.821080\n",
      "75%      106.566394     30.662634   104.870739\n",
      "max      119.200000     82.000000   157.692643\n",
      "[[ 1.          0.23660608 -0.17491194]\n",
      " [ 0.23660608  1.          0.4541125 ]\n",
      " [-0.17491194  0.4541125   1.        ]]\n",
      "[[ 1.          0.22657004 -0.16541427]\n",
      " [ 0.22657004  1.          0.4208305 ]\n",
      " [-0.16541427  0.4208305   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def generate_gaussian_copula_kde(dataframe, n_samples=None):\n",
    "    if n_samples is None:\n",
    "        n_samples = len(dataframe)\n",
    "    \n",
    "    # Step 1: Estimate the marginal distributions using KDE\n",
    "    kde_estimators = []\n",
    "    inverse_cdfs = []\n",
    "    for column in dataframe.columns:\n",
    "        kde = gaussian_kde(dataframe[column])\n",
    "        kde_estimators.append(kde)\n",
    "        \n",
    "        # Compute the inverse CDF using interpolation\n",
    "        x_vals = np.linspace(dataframe[column].min(), dataframe[column].max(), 1000)\n",
    "        cdf_vals = np.array([kde.integrate_box_1d(-np.inf, x) for x in x_vals])\n",
    "        inverse_cdf = interp1d(cdf_vals, x_vals, bounds_error=False, fill_value=(x_vals[0], x_vals[-1]))\n",
    "        inverse_cdfs.append(inverse_cdf)\n",
    "    \n",
    "    # Step 2: Transform data to uniform marginals using the CDF of KDEs\n",
    "    uniform_marginals = np.zeros_like(dataframe.values)\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        cdf_values = np.array([kde_estimators[i].integrate_box_1d(-np.inf, x) for x in dataframe[column]])\n",
    "        uniform_marginals[:, i] = cdf_values\n",
    "    \n",
    "    # Step 3: Transform uniform marginals to standard normal marginals\n",
    "    normal_marginals = norm.ppf(uniform_marginals)\n",
    "    \n",
    "    # Ensure no infinite values (can occur if uniform marginals are exactly 0 or 1)\n",
    "    normal_marginals[np.isinf(normal_marginals)] = np.nan\n",
    "    normal_marginals = np.nan_to_num(normal_marginals)\n",
    "    \n",
    "    # Step 4: Generate new samples from a multivariate normal distribution\n",
    "    mean = np.zeros(dataframe.shape[1])\n",
    "    cov = np.corrcoef(normal_marginals, rowvar=False)\n",
    "    new_samples_normal = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "    \n",
    "    # Step 5: Transform new samples from standard normal to uniform marginals\n",
    "    new_samples_uniform = norm.cdf(new_samples_normal)\n",
    "    \n",
    "    # Step 6: Transform uniform marginals back to original KDE marginals\n",
    "    new_samples = np.zeros_like(new_samples_uniform)\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        new_samples[:, i] = inverse_cdfs[i](new_samples_uniform[:, i])\n",
    "    \n",
    "    # Create a DataFrame with the new samples\n",
    "    generated_dataframe = pd.DataFrame(new_samples, columns=dataframe.columns)\n",
    "    \n",
    "    return generated_dataframe\n",
    "\n",
    "generated_df = generate_gaussian_copula_kde(y_train, 1000)\n",
    "\n",
    "\n",
    "\n",
    "print(y_train.describe())\n",
    "print(generated_df.describe())\n",
    "print(np.corrcoef(y_train,rowvar=False))\n",
    "print(np.corrcoef(generated_df,rowvar=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ada41db-7d90-4e58-bd80-a5efb32d93e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>launch_speed</th>\n",
       "      <th>launch_angle</th>\n",
       "      <th>spray_angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67.814645</td>\n",
       "      <td>1.297331</td>\n",
       "      <td>60.872804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96.506248</td>\n",
       "      <td>53.439815</td>\n",
       "      <td>112.734123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118.120964</td>\n",
       "      <td>25.996291</td>\n",
       "      <td>68.123344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>114.929062</td>\n",
       "      <td>34.710473</td>\n",
       "      <td>115.199989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97.430502</td>\n",
       "      <td>4.508836</td>\n",
       "      <td>106.500743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>86.967676</td>\n",
       "      <td>42.403606</td>\n",
       "      <td>106.387742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>80.668404</td>\n",
       "      <td>13.038609</td>\n",
       "      <td>71.562481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>86.403163</td>\n",
       "      <td>-15.873379</td>\n",
       "      <td>133.296247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>84.012611</td>\n",
       "      <td>-23.981640</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>112.402471</td>\n",
       "      <td>13.930436</td>\n",
       "      <td>90.573596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     launch_speed  launch_angle  spray_angle\n",
       "0       67.814645      1.297331    60.872804\n",
       "1       96.506248     53.439815   112.734123\n",
       "2      118.120964     25.996291    68.123344\n",
       "3      114.929062     34.710473   115.199989\n",
       "4       97.430502      4.508836   106.500743\n",
       "..            ...           ...          ...\n",
       "995     86.967676     42.403606   106.387742\n",
       "996     80.668404     13.038609    71.562481\n",
       "997     86.403163    -15.873379   133.296247\n",
       "998     84.012611    -23.981640    49.000000\n",
       "999    112.402471     13.930436    90.573596\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5652bdca-9d4a-4d2d-89db-1ab67cd1ecac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.85874961,  0.97369666, ..., -0.9748226 ,\n",
       "        -0.87770223,  0.95220237],\n",
       "       [-0.85874961,  1.        , -0.95291002, ...,  0.9513834 ,\n",
       "         0.99926964, -0.66118303],\n",
       "       [ 0.97369666, -0.95291002,  1.        , ..., -0.99998753,\n",
       "        -0.96380206,  0.85755595],\n",
       "       ...,\n",
       "       [-0.9748226 ,  0.9513834 , -0.99998753, ...,  1.        ,\n",
       "         0.9624583 , -0.86011459],\n",
       "       [-0.87770223,  0.99926964, -0.96380206, ...,  0.9624583 ,\n",
       "         1.        , -0.68936795],\n",
       "       [ 0.95220237, -0.66118303,  0.85755595, ..., -0.86011459,\n",
       "        -0.68936795,  1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(copula_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bf04903-f756-4299-8353-63fb9b597866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    391.000000\n",
       "mean      94.802302\n",
       "std       14.500436\n",
       "min       48.000000\n",
       "25%       84.100000\n",
       "50%       97.400000\n",
       "75%      106.300000\n",
       "max      119.200000\n",
       "Name: launch_speed, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train['launch_speed'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87be683c-1275-4be2-9f3f-ee6b7ad2a5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 21:46:44,288 - sdv.metadata.single_table - INFO - Detected metadata:\n",
      "2024-05-22 21:46:44,289 - sdv.metadata.single_table - INFO - {\n",
      "    \"columns\": {\n",
      "        \"launch_speed\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"launch_angle\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"spray_angle\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        }\n",
      "    },\n",
      "    \"METADATA_SPEC_VERSION\": \"SINGLE_TABLE_V1\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Data Validity: |█████████████████████████████████████████████████████| 3/3 [00:00<00:00, 904.20it/s]|\n",
      "Data Validity Score: 100.0%\n",
      "\n",
      "(2/2) Evaluating Data Structure: |████████████████████████████████████████████████████| 1/1 [00:00<00:00, 523.37it/s]|\n",
      "Data Structure Score: 100.0%\n",
      "\n",
      "Overall Score (Average): 100.0%\n",
      "\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |█████████████████████████████████████████████████████| 3/3 [00:00<00:00, 594.04it/s]|\n",
      "Column Shapes Score: 95.76%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |████████████████████████████████████████████████| 3/3 [00:00<00:00, 178.14it/s]|\n",
      "Column Pair Trends Score: 99.12%\n",
      "\n",
      "Overall Score (Average): 97.44%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_12.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_12.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_12.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 21:46:46,208 - sdv.metadata.single_table - INFO - Detected metadata:\n",
      "2024-05-22 21:46:46,208 - sdv.metadata.single_table - INFO - {\n",
      "    \"columns\": {\n",
      "        \"launch_speed\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"launch_angle\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"spray_angle\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        }\n",
      "    },\n",
      "    \"METADATA_SPEC_VERSION\": \"SINGLE_TABLE_V1\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Data Validity: |████████████████████████████████████████████████████| 3/3 [00:00<00:00, 1125.79it/s]|\n",
      "Data Validity Score: 95.63%\n",
      "\n",
      "(2/2) Evaluating Data Structure: |████████████████████████████████████████████████████| 1/1 [00:00<00:00, 543.44it/s]|\n",
      "Data Structure Score: 100.0%\n",
      "\n",
      "Overall Score (Average): 97.82%\n",
      "\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |█████████████████████████████████████████████████████| 3/3 [00:00<00:00, 717.06it/s]|\n",
      "Column Shapes Score: 86.2%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |████████████████████████████████████████████████| 3/3 [00:00<00:00, 168.71it/s]|\n",
      "Column Pair Trends Score: 92.46%\n",
      "\n",
      "Overall Score (Average): 89.33%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_12.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_12.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_12.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from bisect import bisect\n",
    "from copy import deepcopy\n",
    "from sdv.evaluation.single_table import run_diagnostic, evaluate_quality\n",
    "from sdv.evaluation.single_table import get_column_plot\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "#X_train = X_train[['release_speed', 'distance_factor']]\n",
    "#X_test = X_test[['release_speed', 'distance_factor']]\n",
    "\n",
    "def fit_regressors(X, y):\n",
    "    regressors = {}\n",
    "    #fit regressors\n",
    "    for col in y.columns:\n",
    "        param_grid = {\n",
    "            'max_depth': [3, 5],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [1]\n",
    "        }\n",
    "        tree = DecisionTreeRegressor()\n",
    "        grid_search = GridSearchCV(estimator=tree, param_grid=param_grid, cv=5, scoring='r2')\n",
    "        grid_search.fit(X, y[col])\n",
    "        best_tree = grid_search.best_estimator_\n",
    "        regressors[col] = deepcopy(best_tree)\n",
    "    return regressors\n",
    "\n",
    "\n",
    "def fit_regressors2(X, y):\n",
    "    regressors = {}\n",
    "    # Fit regressors\n",
    "    for col in y.columns:\n",
    "        param_grid = {\n",
    "            'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "        }\n",
    "        ridge = Ridge()\n",
    "        grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=5, scoring='r2')\n",
    "        grid_search.fit(X, y[col])\n",
    "        best_ridge = grid_search.best_estimator_\n",
    "        regressors[col] = deepcopy(best_ridge)\n",
    "    return regressors\n",
    "\n",
    "def regressor_diagnostics(regressors, X, y):\n",
    "    for col in y.columns:\n",
    "        y_pred = regressors[col].predict(X)\n",
    "        r2 = r2_score(y[col], y_pred)\n",
    "        mse = mean_squared_error(y[col], y_pred)\n",
    "        print(f'{col}: r2 {r2:.2f}, mse {mse}')\n",
    "    \n",
    "def get_density(X):\n",
    "    pdf, x = np.histogram(X, np.linspace(min(X.values)[0], max(X.values)[0], 1_001), density=True)\n",
    "    cdf = np.cumsum(pdf) * (x[1] - x[0])\n",
    "    return x, pdf, cdf\n",
    "    \n",
    "def generate_sample(quantile_func, n=100_000):\n",
    "    return quantile_func(np.random.uniform(size=n))\n",
    "    \n",
    "def get_quantile_func(X):\n",
    "    x, pdf, cdf = get_density(X)\n",
    "    quantile = lambda val: x[bisect(cdf, val)] \n",
    "    return np.vectorize(quantile)\n",
    "\n",
    "def compute_resids(regressors, X_train, y_train):\n",
    "    resids = {}\n",
    "    for col in y_train.columns:\n",
    "        resids[col] = y_train[col] - regressors[col].predict(X_train)\n",
    "    resids = pd.DataFrame(resids)\n",
    "    return resids\n",
    "\n",
    "def generate_joint_samples(y_train, n_samples=10_000):\n",
    "\n",
    "    #compute resids & get marginal quantile functions\n",
    "    #resids = compute_resids(regressors, X_train, y_train)\n",
    "\n",
    "    #get quantile functions\n",
    "    quantile_funcs = {}\n",
    "    for col in y_train.columns:\n",
    "        quantile_funcs[col] = get_quantile_func(y_train[[col]])\n",
    "\n",
    "    #get correlation matrix\n",
    "    corr_matrix = np.corrcoef(y_train, rowvar=False)\n",
    "    # Step 1: Generate correlated normal samples\n",
    "    normal_samples = np.random.multivariate_normal(np.zeros(3), corr_matrix, size=n_samples)\n",
    "    # Step 2: Transform normal samples to uniform using the normal CDF\n",
    "    uniform_samples = stats.norm.cdf(normal_samples)\n",
    "    # Step 3: map back to desired space\n",
    "    joint_samples = {}\n",
    "    for idx, col in enumerate(y_train.columns):\n",
    "        joint_samples[col] = quantile_funcs[col](uniform_samples[:, idx])\n",
    "    return pd.DataFrame(joint_samples)\n",
    "\n",
    "def run_fit_evaluation(real_data, generated_data):\n",
    "    meta = SingleTableMetadata()\n",
    "    meta.detect_from_dataframe(generated_data)\n",
    "    \n",
    "    # 1. perform basic validity checks\n",
    "    diagnostic = run_diagnostic(real_data, generated_data, meta)\n",
    "    \n",
    "    # 2. measure the statistical similarity\n",
    "    quality_report = evaluate_quality(real_data, generated_data, meta)\n",
    "    \n",
    "    # 3. plot the data\n",
    "    for col in y_test.columns:\n",
    "        fig = get_column_plot(\n",
    "            real_data=real_data,\n",
    "            synthetic_data=generated_data,\n",
    "            metadata=meta,\n",
    "            column_name=col\n",
    "        )\n",
    "        fig.show()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "def predict_regressors(regressors, X):\n",
    "    regressed_data = {}\n",
    "    for col in regressors.keys():\n",
    "        regressed_data[col] = regressors[col].predict(X)\n",
    "\n",
    "    return pd.DataFrame(regressed_data)\n",
    "        \n",
    "\n",
    "#fit regressors, get residuals\n",
    "#regressors = fit_regressors(X_train, y_train)\n",
    "\n",
    "#regressor_diagnostics(regressors, X_train, y_train)\n",
    "#regressor_diagnostics(regressors, X_test, y_test)\n",
    "\n",
    "#sample_res = generate_joint_residual_samples(regressors, X_train, y_train, n_samples=len(y_test))\n",
    "\n",
    "#print(sample_res)\n",
    "\n",
    "#pred_data = predict_regressors(regressors, X_test) + sample_res\n",
    "\n",
    "#y_pred = generate_joint_samples(y_train)\n",
    "\n",
    "run_fit_evaluation(y_train, generated_df)\n",
    "run_fit_evaluation(y_test, generated_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36fdd31a-b607-4e6b-9dad-ec932dc93877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       launch_speed  launch_angle  spray_angle\n",
      "count     15.000000     15.000000    15.000000\n",
      "mean      91.440000     21.066667    98.400000\n",
      "std       10.905949     27.824621    22.516026\n",
      "min       72.700000    -41.000000    48.000000\n",
      "25%       86.750000     13.000000    86.000000\n",
      "50%       93.700000     24.000000   104.000000\n",
      "75%       99.750000     37.000000   117.000000\n",
      "max      107.600000     67.000000   128.000000 \n",
      "\n",
      "        launch_speed  launch_angle  spray_angle\n",
      "count    135.000000    135.000000   135.000000\n",
      "mean      89.818519     24.600000    99.437037\n",
      "std       12.395866     24.043245    27.622349\n",
      "min       51.600000    -45.000000    27.000000\n",
      "25%       83.750000      9.000000    79.000000\n",
      "50%       91.100000     24.000000   106.000000\n",
      "75%       99.850000     39.000000   118.000000\n",
      "max      109.300000     85.000000   232.000000 \n",
      "\n",
      "        launch_speed  launch_angle   spray_angle\n",
      "count  10000.000000  10000.000000  10000.000000\n",
      "mean      89.632251     24.498754     98.935668\n",
      "std       12.360896     23.848670     27.131716\n",
      "min       51.600000    -45.000000     27.000000\n",
      "25%       83.681200      8.950000     78.865000\n",
      "50%       91.066800     23.900000    103.875000\n",
      "75%       99.779500     38.980000    116.995000\n",
      "max      109.242300     84.870000    231.795000\n"
     ]
    }
   ],
   "source": [
    "print(y_test.describe(), '\\n\\n',\n",
    "      y_train.describe(), '\\n\\n',\n",
    "      y_pred.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43fb7cd5-6208-4aa8-9116-2d58f7ddb868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 17:54:09,623 - sdv.metadata.single_table - INFO - Detected metadata:\n",
      "2024-05-19 17:54:09,626 - sdv.metadata.single_table - INFO - {\n",
      "    \"columns\": {\n",
      "        \"launch_speed\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"launch_angle\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"spray_angle\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        }\n",
      "    },\n",
      "    \"METADATA_SPEC_VERSION\": \"SINGLE_TABLE_V1\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Data Validity: |█████████████████████████████████████████████████████| 3/3 [00:00<00:00, 496.39it/s]|\n",
      "Data Validity Score: 100.0%\n",
      "\n",
      "(2/2) Evaluating Data Structure: |████████████████████████████████████████████████████| 1/1 [00:00<00:00, 469.42it/s]|\n",
      "Data Structure Score: 100.0%\n",
      "\n",
      "Overall Score (Average): 100.0%\n",
      "\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |█████████████████████████████████████████████████████| 3/3 [00:00<00:00, 249.76it/s]|\n",
      "Column Shapes Score: 97.65%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |████████████████████████████████████████████████| 3/3 [00:00<00:00, 155.61it/s]|\n",
      "Column Pair Trends Score: 99.44%\n",
      "\n",
      "Overall Score (Average): 98.55%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_18.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_18.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_18.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_fit_evaluation(y_train, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea52c09b-20a2-45a4-af08-a9be9ef8b953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.32003678 -0.10351609]\n",
      " [ 0.32003678  1.          0.42257853]\n",
      " [-0.10351609  0.42257853  1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.31553617, -0.08704386],\n",
       "       [ 0.31553617,  1.        ,  0.4141389 ],\n",
       "       [-0.08704386,  0.4141389 ,  1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.corrcoef(y_train, rowvar=False))\n",
    "np.corrcoef(y_pred, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd3c46dd-e06b-4c33-b776-9b68d264a529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>launch_speed</th>\n",
       "      <th>launch_angle</th>\n",
       "      <th>spray_angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>92.994955</td>\n",
       "      <td>8.362973</td>\n",
       "      <td>89.079298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.654060</td>\n",
       "      <td>30.289969</td>\n",
       "      <td>22.303942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>-81.000000</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>84.638400</td>\n",
       "      <td>-10.095000</td>\n",
       "      <td>69.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>96.665300</td>\n",
       "      <td>8.976000</td>\n",
       "      <td>86.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>104.146600</td>\n",
       "      <td>27.884000</td>\n",
       "      <td>105.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>115.605300</td>\n",
       "      <td>81.837000</td>\n",
       "      <td>146.901000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       launch_speed  launch_angle   spray_angle\n",
       "count  10000.000000  10000.000000  10000.000000\n",
       "mean      92.994955      8.362973     89.079298\n",
       "std       14.654060     30.289969     22.303942\n",
       "min       21.000000    -81.000000     48.000000\n",
       "25%       84.638400    -10.095000     69.978000\n",
       "50%       96.665300      8.976000     86.907000\n",
       "75%      104.146600     27.884000    105.915000\n",
       "max      115.605300     81.837000    146.901000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1447c355-3058-4d0a-a55f-2b5e1211aaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>launch_speed</th>\n",
       "      <th>launch_angle</th>\n",
       "      <th>spray_angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>92.833566</td>\n",
       "      <td>7.606061</td>\n",
       "      <td>88.948718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.811679</td>\n",
       "      <td>30.041576</td>\n",
       "      <td>22.255256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>-81.000000</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>-11.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>96.600000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>104.100000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>115.700000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>147.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       launch_speed  launch_angle  spray_angle\n",
       "count    429.000000    429.000000   429.000000\n",
       "mean      92.833566      7.606061    88.948718\n",
       "std       14.811679     30.041576    22.255256\n",
       "min       21.000000    -81.000000    48.000000\n",
       "25%       84.600000    -11.000000    70.000000\n",
       "50%       96.600000      8.000000    87.000000\n",
       "75%      104.100000     27.000000   106.000000\n",
       "max      115.700000     82.000000   147.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
