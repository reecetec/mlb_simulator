{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db2abde-1118-42e8-a624-d42bb5014ebf",
   "metadata": {},
   "source": [
    "# Generate Bat Outcome\n",
    " - Given the batter has made contact on the given pitch characteristics, what is the batting outcome?\n",
    " - I.e., what is the launch_speed_angle, and what is the hit_location\n",
    "\n",
    "## Potential Difficulties:\n",
    " - How to factor in bunting?\n",
    "\n",
    "$$\\alpha$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35827e83-2d66-42cc-97c9-4b5898d22fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from src.features import build_features as f\n",
    "from src.data import data_utils as du\n",
    "from importlib import reload\n",
    "reload(f)\n",
    "\n",
    "vladdy = 665489\n",
    "soto = 665742\n",
    "schneider = 676914\n",
    "biggio = 624415\n",
    "batter = soto\n",
    "X_train, y_train, X_test, y_test, encoders = f.get_hit_outcome_dataset(batter, split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4cc5f74-cf19-4810-a14b-2b8989d3b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandwidth: scott, Average Log Likelihood: -386.3460999213018\n",
      "Bandwidth: silverman, Average Log Likelihood: -383.07674515533836\n",
      "Bandwidth: 0.1, Average Log Likelihood: -381.24942209010254\n",
      "Bandwidth: 0.2, Average Log Likelihood: -383.1022594355098\n",
      "Bandwidth: 0.5, Average Log Likelihood: -386.6951514332781\n",
      "Bandwidth: 1.0, Average Log Likelihood: -393.25794008776904\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def evaluate_kde_bandwidth(dataframe, bandwidth):\n",
    "    kde_estimators = []\n",
    "    inverse_cdfs = []\n",
    "    for column in dataframe.columns:\n",
    "        kde = gaussian_kde(dataframe[column], bw_method=bandwidth)\n",
    "        kde_estimators.append(kde)\n",
    "        \n",
    "        # Compute the inverse CDF using interpolation\n",
    "        x_vals = np.linspace(dataframe[column].min(), dataframe[column].max(), 1000)\n",
    "        cdf_vals = np.array([kde.integrate_box_1d(-np.inf, x) for x in x_vals])\n",
    "        inverse_cdf = interp1d(cdf_vals, x_vals, bounds_error=False, fill_value=(x_vals[0], x_vals[-1]))\n",
    "        inverse_cdfs.append(inverse_cdf)\n",
    "    \n",
    "    return kde_estimators, inverse_cdfs\n",
    "\n",
    "def generate_gaussian_copula_kde(dataframe, kde_estimators, inverse_cdfs, n_samples=None):\n",
    "    if n_samples is None:\n",
    "        n_samples = len(dataframe)\n",
    "    \n",
    "    # Step 2: Transform data to uniform marginals using the CDF of KDEs\n",
    "    uniform_marginals = np.zeros_like(dataframe.values)\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        cdf_values = np.array([kde_estimators[i].integrate_box_1d(-np.inf, x) for x in dataframe[column]])\n",
    "        uniform_marginals[:, i] = cdf_values\n",
    "    \n",
    "    # Step 3: Transform uniform marginals to standard normal marginals\n",
    "    normal_marginals = norm.ppf(uniform_marginals)\n",
    "    \n",
    "    # Ensure no infinite values (can occur if uniform marginals are exactly 0 or 1)\n",
    "    normal_marginals[np.isinf(normal_marginals)] = np.nan\n",
    "    normal_marginals = np.nan_to_num(normal_marginals)\n",
    "    \n",
    "    # Step 4: Generate new samples from a multivariate normal distribution\n",
    "    mean = np.zeros(dataframe.shape[1])\n",
    "    cov = np.corrcoef(normal_marginals, rowvar=False)\n",
    "    new_samples_normal = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "    \n",
    "    # Step 5: Transform new samples from standard normal to uniform marginals\n",
    "    new_samples_uniform = norm.cdf(new_samples_normal)\n",
    "    \n",
    "    # Step 6: Transform uniform marginals back to original KDE marginals\n",
    "    new_samples = np.zeros_like(new_samples_uniform)\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        new_samples[:, i] = inverse_cdfs[i](new_samples_uniform[:, i])\n",
    "    \n",
    "    # Create a DataFrame with the new samples\n",
    "    generated_dataframe = pd.DataFrame(new_samples, columns=dataframe.columns)\n",
    "    \n",
    "    return generated_dataframe\n",
    "\n",
    "def cross_validate_kde(data, bandwidths, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    results = []\n",
    "    \n",
    "    for bandwidth in bandwidths:\n",
    "        log_likelihoods = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(data):\n",
    "            train_data, test_data = data.iloc[train_index], data.iloc[test_index]\n",
    "            kde_estimators, inverse_cdfs = evaluate_kde_bandwidth(train_data, bandwidth)\n",
    "            generated_data = generate_gaussian_copula_kde(train_data, kde_estimators, inverse_cdfs, n_samples=len(test_data))\n",
    "            \n",
    "            # Evaluate the generated data\n",
    "            for column in data.columns:\n",
    "                kde = gaussian_kde(generated_data[column])\n",
    "                log_likelihood = kde.logpdf(test_data[column]).sum()\n",
    "                log_likelihoods.append(log_likelihood)\n",
    "        \n",
    "        avg_log_likelihood = np.mean(log_likelihoods)\n",
    "        results.append((bandwidth, avg_log_likelihood))\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "bandwidths = ['scott', 'silverman', 0.1, 0.2, 0.5, 1.0]\n",
    "results = cross_validate_kde(y_train, bandwidths)\n",
    "\n",
    "for bandwidth, avg_log_likelihood in results:\n",
    "    print(f'Bandwidth: {bandwidth}, Average Log Likelihood: {avg_log_likelihood}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64097c09-7fd9-4ed6-a309-4c5f296602d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bandwidth: 0.1\n",
      "Test Set Log Likelihood: -211.57732259582986\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def evaluate_kde_bandwidth(dataframe, bandwidth):\n",
    "    kde_estimators = []\n",
    "    inverse_cdfs = []\n",
    "    for column in dataframe.columns:\n",
    "        kde = gaussian_kde(dataframe[column], bw_method=bandwidth)\n",
    "        kde_estimators.append(kde)\n",
    "        \n",
    "        # Compute the inverse CDF using interpolation\n",
    "        x_vals = np.linspace(dataframe[column].min(), dataframe[column].max(), 1000)\n",
    "        cdf_vals = np.array([kde.integrate_box_1d(-np.inf, x) for x in x_vals])\n",
    "        inverse_cdf = interp1d(cdf_vals, x_vals, bounds_error=False, fill_value=(x_vals[0], x_vals[-1]))\n",
    "        inverse_cdfs.append(inverse_cdf)\n",
    "    \n",
    "    return kde_estimators, inverse_cdfs\n",
    "\n",
    "def generate_gaussian_copula_kde(dataframe, kde_estimators, inverse_cdfs, n_samples=None):\n",
    "    if n_samples is None:\n",
    "        n_samples = len(dataframe)\n",
    "    \n",
    "    # Step 2: Transform data to uniform marginals using the CDF of KDEs\n",
    "    uniform_marginals = np.zeros_like(dataframe.values)\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        cdf_values = np.array([kde_estimators[i].integrate_box_1d(-np.inf, x) for x in dataframe[column]])\n",
    "        uniform_marginals[:, i] = cdf_values\n",
    "    \n",
    "    # Step 3: Transform uniform marginals to standard normal marginals\n",
    "    normal_marginals = norm.ppf(uniform_marginals)\n",
    "    \n",
    "    # Ensure no infinite values (can occur if uniform marginals are exactly 0 or 1)\n",
    "    normal_marginals[np.isinf(normal_marginals)] = np.nan\n",
    "    normal_marginals = np.nan_to_num(normal_marginals)\n",
    "    \n",
    "    # Step 4: Generate new samples from a multivariate normal distribution\n",
    "    mean = np.zeros(dataframe.shape[1])\n",
    "    cov = np.corrcoef(normal_marginals, rowvar=False)\n",
    "    new_samples_normal = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "    \n",
    "    # Step 5: Transform new samples from standard normal to uniform marginals\n",
    "    new_samples_uniform = norm.cdf(new_samples_normal)\n",
    "    \n",
    "    # Step 6: Transform uniform marginals back to original KDE marginals\n",
    "    new_samples = np.zeros_like(new_samples_uniform)\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        new_samples[:, i] = inverse_cdfs[i](new_samples_uniform[:, i])\n",
    "    \n",
    "    # Create a DataFrame with the new samples\n",
    "    generated_dataframe = pd.DataFrame(new_samples, columns=dataframe.columns)\n",
    "    \n",
    "    return generated_dataframe\n",
    "\n",
    "def cross_validate_kde(data, bandwidths, n_splits=5):\n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    results = []\n",
    "    \n",
    "    for bandwidth in bandwidths:\n",
    "        train_log_likelihoods = []\n",
    "        \n",
    "        for train_index, test_index in kf.split(data):\n",
    "            train_data = data.iloc[train_index]\n",
    "            kde_estimators, inverse_cdfs = evaluate_kde_bandwidth(train_data, bandwidth)\n",
    "            generated_data = generate_gaussian_copula_kde(train_data, kde_estimators, inverse_cdfs, n_samples=len(train_data))\n",
    "            \n",
    "            # Evaluate the generated data\n",
    "            log_likelihoods = []\n",
    "            for column in data.columns:\n",
    "                kde = gaussian_kde(generated_data[column])\n",
    "                log_likelihood = kde.logpdf(train_data[column]).sum()\n",
    "                log_likelihoods.append(log_likelihood)\n",
    "            train_log_likelihoods.append(np.mean(log_likelihoods))\n",
    "        \n",
    "        avg_train_log_likelihood = np.mean(train_log_likelihoods)\n",
    "        results.append((bandwidth, avg_train_log_likelihood))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_on_test_set(data, test_data, best_bandwidth):\n",
    "    kde_estimators, inverse_cdfs = evaluate_kde_bandwidth(data, best_bandwidth)\n",
    "    generated_data = generate_gaussian_copula_kde(data, kde_estimators, inverse_cdfs, n_samples=len(test_data))\n",
    "    \n",
    "    log_likelihoods = []\n",
    "    for column in test_data.columns:\n",
    "        kde = gaussian_kde(generated_data[column])\n",
    "        log_likelihood = kde.logpdf(test_data[column]).sum()\n",
    "        log_likelihoods.append(log_likelihood)\n",
    "    \n",
    "    avg_test_log_likelihood = np.mean(log_likelihoods)\n",
    "    return avg_test_log_likelihood\n",
    "\n",
    "\n",
    "bandwidths = ['scott', 'silverman', 0.1, 0.2, 0.5, 1.0]\n",
    "results = cross_validate_kde(y_train, bandwidths)\n",
    "\n",
    "# Find the best bandwidth based on the highest average log likelihood\n",
    "best_bandwidth = max(results, key=lambda x: x[1])[0]\n",
    "print(f'Best Bandwidth: {best_bandwidth}')\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_log_likelihood = evaluate_on_test_set(y_train, y_test, best_bandwidth)\n",
    "print(f'Test Set Log Likelihood: {test_log_likelihood}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5df4d6b-dc1a-4e0b-bd11-372e433ca809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       launch_speed  launch_angle  spray_angle\n",
      "count    429.000000    429.000000   429.000000\n",
      "mean      92.833566      7.606061    88.948718\n",
      "std       14.811679     30.041576    22.255256\n",
      "min       21.000000    -81.000000    48.000000\n",
      "25%       84.600000    -11.000000    70.000000\n",
      "50%       96.600000      8.000000    87.000000\n",
      "75%      104.100000     27.000000   106.000000\n",
      "max      115.700000     82.000000   147.000000\n",
      "       launch_speed  launch_angle  spray_angle\n",
      "count      1.000000      1.000000     1.000000\n",
      "mean     102.233208     22.032888    72.469638\n",
      "std             NaN           NaN          NaN\n",
      "min      102.233208     22.032888    72.469638\n",
      "25%      102.233208     22.032888    72.469638\n",
      "50%      102.233208     22.032888    72.469638\n",
      "75%      102.233208     22.032888    72.469638\n",
      "max      102.233208     22.032888    72.469638\n",
      "[[ 1.          0.32003678 -0.10351609]\n",
      " [ 0.32003678  1.          0.42257853]\n",
      " [-0.10351609  0.42257853  1.        ]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "def generate_gaussian_copula_kde(dataframe, n_samples=None):\n",
    "    if n_samples is None:\n",
    "        n_samples = len(dataframe)\n",
    "    \n",
    "    # Step 1: Estimate the marginal distributions using KDE\n",
    "    kde_estimators = []\n",
    "    inverse_cdfs = []\n",
    "    for column in dataframe.columns:\n",
    "        kde = gaussian_kde(dataframe[column])\n",
    "        kde_estimators.append(kde)\n",
    "        \n",
    "        # Compute the inverse CDF using interpolation\n",
    "        x_vals = np.linspace(dataframe[column].min(), dataframe[column].max(), 1000)\n",
    "        cdf_vals = np.array([kde.integrate_box_1d(-np.inf, x) for x in x_vals])\n",
    "        inverse_cdf = interp1d(cdf_vals, x_vals, bounds_error=False, fill_value=(x_vals[0], x_vals[-1]))\n",
    "        inverse_cdfs.append(inverse_cdf)\n",
    "    \n",
    "    # Step 2: Transform data to uniform marginals using the CDF of KDEs\n",
    "    uniform_marginals = np.zeros_like(dataframe.values)\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        cdf_values = np.array([kde_estimators[i].integrate_box_1d(-np.inf, x) for x in dataframe[column]])\n",
    "        uniform_marginals[:, i] = cdf_values\n",
    "    \n",
    "    # Step 3: Transform uniform marginals to standard normal marginals\n",
    "    normal_marginals = norm.ppf(uniform_marginals)\n",
    "    \n",
    "    # Ensure no infinite values (can occur if uniform marginals are exactly 0 or 1)\n",
    "    normal_marginals[np.isinf(normal_marginals)] = np.nan\n",
    "    normal_marginals = np.nan_to_num(normal_marginals)\n",
    "    \n",
    "    # Step 4: Generate new samples from a multivariate normal distribution\n",
    "    mean = np.zeros(dataframe.shape[1])\n",
    "    cov = np.corrcoef(normal_marginals, rowvar=False)\n",
    "    new_samples_normal = np.random.multivariate_normal(mean, cov, size=n_samples)\n",
    "    \n",
    "    # Step 5: Transform new samples from standard normal to uniform marginals\n",
    "    new_samples_uniform = norm.cdf(new_samples_normal)\n",
    "    \n",
    "    # Step 6: Transform uniform marginals back to original KDE marginals\n",
    "    new_samples = np.zeros_like(new_samples_uniform)\n",
    "    for i, column in enumerate(dataframe.columns):\n",
    "        new_samples[:, i] = inverse_cdfs[i](new_samples_uniform[:, i])\n",
    "    \n",
    "    # Create a DataFrame with the new samples\n",
    "    generated_dataframe = pd.DataFrame(new_samples, columns=dataframe.columns)\n",
    "    \n",
    "    return generated_dataframe\n",
    "\n",
    "generated_df = generate_gaussian_copula_kde(y_train, 1)\n",
    "\n",
    "print(y_train.describe())\n",
    "print(generated_df.describe())\n",
    "print(np.corrcoef(y_train,rowvar=False))\n",
    "print(np.corrcoef(generated_df,rowvar=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5652bdca-9d4a-4d2d-89db-1ab67cd1ecac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.85874961,  0.97369666, ..., -0.9748226 ,\n",
       "        -0.87770223,  0.95220237],\n",
       "       [-0.85874961,  1.        , -0.95291002, ...,  0.9513834 ,\n",
       "         0.99926964, -0.66118303],\n",
       "       [ 0.97369666, -0.95291002,  1.        , ..., -0.99998753,\n",
       "        -0.96380206,  0.85755595],\n",
       "       ...,\n",
       "       [-0.9748226 ,  0.9513834 , -0.99998753, ...,  1.        ,\n",
       "         0.9624583 , -0.86011459],\n",
       "       [-0.87770223,  0.99926964, -0.96380206, ...,  0.9624583 ,\n",
       "         1.        , -0.68936795],\n",
       "       [ 0.95220237, -0.66118303,  0.85755595, ..., -0.86011459,\n",
       "        -0.68936795,  1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(copula_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87be683c-1275-4be2-9f3f-ee6b7ad2a5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 11:37:26,308 - sdv.metadata.single_table - INFO - Detected metadata:\n",
      "2024-05-20 11:37:26,309 - sdv.metadata.single_table - INFO - {\n",
      "    \"columns\": {\n",
      "        \"launch_speed\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"launch_angle\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"spray_angle\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        }\n",
      "    },\n",
      "    \"METADATA_SPEC_VERSION\": \"SINGLE_TABLE_V1\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Data Validity: |█████████████████████████████████████████████████████| 3/3 [00:00<00:00, 962.95it/s]|\n",
      "Data Validity Score: 100.0%\n",
      "\n",
      "(2/2) Evaluating Data Structure: |████████████████████████████████████████████████████| 1/1 [00:00<00:00, 687.14it/s]|\n",
      "Data Structure Score: 100.0%\n",
      "\n",
      "Overall Score (Average): 100.0%\n",
      "\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |█████████████████████████████████████████████████████| 3/3 [00:00<00:00, 782.08it/s]|\n",
      "Column Shapes Score: 93.55%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |████████████████████████████████████████████████| 3/3 [00:00<00:00, 182.02it/s]|\n",
      "Column Pair Trends Score: 97.16%\n",
      "\n",
      "Overall Score (Average): 95.36%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 11:37:28,057 - sdv.metadata.single_table - INFO - Detected metadata:\n",
      "2024-05-20 11:37:28,059 - sdv.metadata.single_table - INFO - {\n",
      "    \"columns\": {\n",
      "        \"launch_speed\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"launch_angle\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"spray_angle\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        }\n",
      "    },\n",
      "    \"METADATA_SPEC_VERSION\": \"SINGLE_TABLE_V1\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Data Validity: |█████████████████████████████████████████████████████| 3/3 [00:00<00:00, 314.62it/s]|\n",
      "Data Validity Score: 94.33%\n",
      "\n",
      "(2/2) Evaluating Data Structure: |████████████████████████████████████████████████████| 1/1 [00:00<00:00, 444.97it/s]|\n",
      "Data Structure Score: 100.0%\n",
      "\n",
      "Overall Score (Average): 97.16%\n",
      "\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |█████████████████████████████████████████████████████| 3/3 [00:00<00:00, 517.84it/s]|\n",
      "Column Shapes Score: 82.64%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |████████████████████████████████████████████████| 3/3 [00:00<00:00, 173.82it/s]|\n",
      "Column Pair Trends Score: 91.02%\n",
      "\n",
      "Overall Score (Average): 86.83%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_8.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from bisect import bisect\n",
    "from copy import deepcopy\n",
    "from sdv.evaluation.single_table import run_diagnostic, evaluate_quality\n",
    "from sdv.evaluation.single_table import get_column_plot\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from scipy import stats\n",
    "import time\n",
    "\n",
    "#X_train = X_train[['release_speed', 'distance_factor']]\n",
    "#X_test = X_test[['release_speed', 'distance_factor']]\n",
    "\n",
    "def fit_regressors(X, y):\n",
    "    regressors = {}\n",
    "    #fit regressors\n",
    "    for col in y.columns:\n",
    "        param_grid = {\n",
    "            'max_depth': [3, 5],\n",
    "            'min_samples_split': [2],\n",
    "            'min_samples_leaf': [1]\n",
    "        }\n",
    "        tree = DecisionTreeRegressor()\n",
    "        grid_search = GridSearchCV(estimator=tree, param_grid=param_grid, cv=5, scoring='r2')\n",
    "        grid_search.fit(X, y[col])\n",
    "        best_tree = grid_search.best_estimator_\n",
    "        regressors[col] = deepcopy(best_tree)\n",
    "    return regressors\n",
    "\n",
    "\n",
    "def fit_regressors2(X, y):\n",
    "    regressors = {}\n",
    "    # Fit regressors\n",
    "    for col in y.columns:\n",
    "        param_grid = {\n",
    "            'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "        }\n",
    "        ridge = Ridge()\n",
    "        grid_search = GridSearchCV(estimator=ridge, param_grid=param_grid, cv=5, scoring='r2')\n",
    "        grid_search.fit(X, y[col])\n",
    "        best_ridge = grid_search.best_estimator_\n",
    "        regressors[col] = deepcopy(best_ridge)\n",
    "    return regressors\n",
    "\n",
    "def regressor_diagnostics(regressors, X, y):\n",
    "    for col in y.columns:\n",
    "        y_pred = regressors[col].predict(X)\n",
    "        r2 = r2_score(y[col], y_pred)\n",
    "        mse = mean_squared_error(y[col], y_pred)\n",
    "        print(f'{col}: r2 {r2:.2f}, mse {mse}')\n",
    "    \n",
    "def get_density(X):\n",
    "    pdf, x = np.histogram(X, np.linspace(min(X.values)[0], max(X.values)[0], 1_001), density=True)\n",
    "    cdf = np.cumsum(pdf) * (x[1] - x[0])\n",
    "    return x, pdf, cdf\n",
    "    \n",
    "def generate_sample(quantile_func, n=100_000):\n",
    "    return quantile_func(np.random.uniform(size=n))\n",
    "    \n",
    "def get_quantile_func(X):\n",
    "    x, pdf, cdf = get_density(X)\n",
    "    quantile = lambda val: x[bisect(cdf, val)] \n",
    "    return np.vectorize(quantile)\n",
    "\n",
    "def compute_resids(regressors, X_train, y_train):\n",
    "    resids = {}\n",
    "    for col in y_train.columns:\n",
    "        resids[col] = y_train[col] - regressors[col].predict(X_train)\n",
    "    resids = pd.DataFrame(resids)\n",
    "    return resids\n",
    "\n",
    "def generate_joint_samples(y_train, n_samples=10_000):\n",
    "\n",
    "    #compute resids & get marginal quantile functions\n",
    "    #resids = compute_resids(regressors, X_train, y_train)\n",
    "\n",
    "    #get quantile functions\n",
    "    quantile_funcs = {}\n",
    "    for col in y_train.columns:\n",
    "        quantile_funcs[col] = get_quantile_func(y_train[[col]])\n",
    "\n",
    "    #get correlation matrix\n",
    "    corr_matrix = np.corrcoef(y_train, rowvar=False)\n",
    "    # Step 1: Generate correlated normal samples\n",
    "    normal_samples = np.random.multivariate_normal(np.zeros(3), corr_matrix, size=n_samples)\n",
    "    # Step 2: Transform normal samples to uniform using the normal CDF\n",
    "    uniform_samples = stats.norm.cdf(normal_samples)\n",
    "    # Step 3: map back to desired space\n",
    "    joint_samples = {}\n",
    "    for idx, col in enumerate(y_train.columns):\n",
    "        joint_samples[col] = quantile_funcs[col](uniform_samples[:, idx])\n",
    "    return pd.DataFrame(joint_samples)\n",
    "\n",
    "def run_fit_evaluation(real_data, generated_data):\n",
    "    meta = SingleTableMetadata()\n",
    "    meta.detect_from_dataframe(generated_data)\n",
    "    \n",
    "    # 1. perform basic validity checks\n",
    "    diagnostic = run_diagnostic(real_data, generated_data, meta)\n",
    "    \n",
    "    # 2. measure the statistical similarity\n",
    "    quality_report = evaluate_quality(real_data, generated_data, meta)\n",
    "    \n",
    "    # 3. plot the data\n",
    "    for col in y_test.columns:\n",
    "        fig = get_column_plot(\n",
    "            real_data=real_data,\n",
    "            synthetic_data=generated_data,\n",
    "            metadata=meta,\n",
    "            column_name=col\n",
    "        )\n",
    "        fig.show()\n",
    "        time.sleep(0.5)\n",
    "\n",
    "def predict_regressors(regressors, X):\n",
    "    regressed_data = {}\n",
    "    for col in regressors.keys():\n",
    "        regressed_data[col] = regressors[col].predict(X)\n",
    "\n",
    "    return pd.DataFrame(regressed_data)\n",
    "        \n",
    "\n",
    "#fit regressors, get residuals\n",
    "#regressors = fit_regressors(X_train, y_train)\n",
    "\n",
    "#regressor_diagnostics(regressors, X_train, y_train)\n",
    "#regressor_diagnostics(regressors, X_test, y_test)\n",
    "\n",
    "#sample_res = generate_joint_residual_samples(regressors, X_train, y_train, n_samples=len(y_test))\n",
    "\n",
    "#print(sample_res)\n",
    "\n",
    "#pred_data = predict_regressors(regressors, X_test) + sample_res\n",
    "\n",
    "#y_pred = generate_joint_samples(y_train)\n",
    "\n",
    "run_fit_evaluation(y_train, generated_df)\n",
    "run_fit_evaluation(y_test, generated_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36fdd31a-b607-4e6b-9dad-ec932dc93877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       launch_speed  launch_angle  spray_angle\n",
      "count     15.000000     15.000000    15.000000\n",
      "mean      91.440000     21.066667    98.400000\n",
      "std       10.905949     27.824621    22.516026\n",
      "min       72.700000    -41.000000    48.000000\n",
      "25%       86.750000     13.000000    86.000000\n",
      "50%       93.700000     24.000000   104.000000\n",
      "75%       99.750000     37.000000   117.000000\n",
      "max      107.600000     67.000000   128.000000 \n",
      "\n",
      "        launch_speed  launch_angle  spray_angle\n",
      "count    135.000000    135.000000   135.000000\n",
      "mean      89.818519     24.600000    99.437037\n",
      "std       12.395866     24.043245    27.622349\n",
      "min       51.600000    -45.000000    27.000000\n",
      "25%       83.750000      9.000000    79.000000\n",
      "50%       91.100000     24.000000   106.000000\n",
      "75%       99.850000     39.000000   118.000000\n",
      "max      109.300000     85.000000   232.000000 \n",
      "\n",
      "        launch_speed  launch_angle   spray_angle\n",
      "count  10000.000000  10000.000000  10000.000000\n",
      "mean      89.632251     24.498754     98.935668\n",
      "std       12.360896     23.848670     27.131716\n",
      "min       51.600000    -45.000000     27.000000\n",
      "25%       83.681200      8.950000     78.865000\n",
      "50%       91.066800     23.900000    103.875000\n",
      "75%       99.779500     38.980000    116.995000\n",
      "max      109.242300     84.870000    231.795000\n"
     ]
    }
   ],
   "source": [
    "print(y_test.describe(), '\\n\\n',\n",
    "      y_train.describe(), '\\n\\n',\n",
    "      y_pred.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43fb7cd5-6208-4aa8-9116-2d58f7ddb868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 17:54:09,623 - sdv.metadata.single_table - INFO - Detected metadata:\n",
      "2024-05-19 17:54:09,626 - sdv.metadata.single_table - INFO - {\n",
      "    \"columns\": {\n",
      "        \"launch_speed\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"launch_angle\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        },\n",
      "        \"spray_angle\": {\n",
      "            \"sdtype\": \"numerical\"\n",
      "        }\n",
      "    },\n",
      "    \"METADATA_SPEC_VERSION\": \"SINGLE_TABLE_V1\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Data Validity: |█████████████████████████████████████████████████████| 3/3 [00:00<00:00, 496.39it/s]|\n",
      "Data Validity Score: 100.0%\n",
      "\n",
      "(2/2) Evaluating Data Structure: |████████████████████████████████████████████████████| 1/1 [00:00<00:00, 469.42it/s]|\n",
      "Data Structure Score: 100.0%\n",
      "\n",
      "Overall Score (Average): 100.0%\n",
      "\n",
      "Generating report ...\n",
      "\n",
      "(1/2) Evaluating Column Shapes: |█████████████████████████████████████████████████████| 3/3 [00:00<00:00, 249.76it/s]|\n",
      "Column Shapes Score: 97.65%\n",
      "\n",
      "(2/2) Evaluating Column Pair Trends: |████████████████████████████████████████████████| 3/3 [00:00<00:00, 155.61it/s]|\n",
      "Column Pair Trends Score: 99.44%\n",
      "\n",
      "Overall Score (Average): 98.55%\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_18.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_18.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<iframe\n",
       "    scrolling=\"no\"\n",
       "    width=\"100%\"\n",
       "    height=\"545px\"\n",
       "    src=\"iframe_figures/figure_18.html\"\n",
       "    frameborder=\"0\"\n",
       "    allowfullscreen\n",
       "></iframe>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_fit_evaluation(y_train, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea52c09b-20a2-45a4-af08-a9be9ef8b953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.32003678 -0.10351609]\n",
      " [ 0.32003678  1.          0.42257853]\n",
      " [-0.10351609  0.42257853  1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.31553617, -0.08704386],\n",
       "       [ 0.31553617,  1.        ,  0.4141389 ],\n",
       "       [-0.08704386,  0.4141389 ,  1.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.corrcoef(y_train, rowvar=False))\n",
    "np.corrcoef(y_pred, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd3c46dd-e06b-4c33-b776-9b68d264a529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>launch_speed</th>\n",
       "      <th>launch_angle</th>\n",
       "      <th>spray_angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>92.994955</td>\n",
       "      <td>8.362973</td>\n",
       "      <td>89.079298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.654060</td>\n",
       "      <td>30.289969</td>\n",
       "      <td>22.303942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>-81.000000</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>84.638400</td>\n",
       "      <td>-10.095000</td>\n",
       "      <td>69.978000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>96.665300</td>\n",
       "      <td>8.976000</td>\n",
       "      <td>86.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>104.146600</td>\n",
       "      <td>27.884000</td>\n",
       "      <td>105.915000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>115.605300</td>\n",
       "      <td>81.837000</td>\n",
       "      <td>146.901000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       launch_speed  launch_angle   spray_angle\n",
       "count  10000.000000  10000.000000  10000.000000\n",
       "mean      92.994955      8.362973     89.079298\n",
       "std       14.654060     30.289969     22.303942\n",
       "min       21.000000    -81.000000     48.000000\n",
       "25%       84.638400    -10.095000     69.978000\n",
       "50%       96.665300      8.976000     86.907000\n",
       "75%      104.146600     27.884000    105.915000\n",
       "max      115.605300     81.837000    146.901000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1447c355-3058-4d0a-a55f-2b5e1211aaae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>launch_speed</th>\n",
       "      <th>launch_angle</th>\n",
       "      <th>spray_angle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "      <td>429.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>92.833566</td>\n",
       "      <td>7.606061</td>\n",
       "      <td>88.948718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.811679</td>\n",
       "      <td>30.041576</td>\n",
       "      <td>22.255256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>21.000000</td>\n",
       "      <td>-81.000000</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>84.600000</td>\n",
       "      <td>-11.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>96.600000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>104.100000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>106.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>115.700000</td>\n",
       "      <td>82.000000</td>\n",
       "      <td>147.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       launch_speed  launch_angle  spray_angle\n",
       "count    429.000000    429.000000   429.000000\n",
       "mean      92.833566      7.606061    88.948718\n",
       "std       14.811679     30.041576    22.255256\n",
       "min       21.000000    -81.000000    48.000000\n",
       "25%       84.600000    -11.000000    70.000000\n",
       "50%       96.600000      8.000000    87.000000\n",
       "75%      104.100000     27.000000   106.000000\n",
       "max      115.700000     82.000000   147.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
